<h2 class="govuk-heading-m">Working with performance analysts</h2>
<h2 class="govuk-heading-s">What is a performance analyst?</h2>

<p class="govuk-body">Performance analysts develop success metrics and analyse the performance of a product against them. They help you find evidence to show if your service is meeting user needs, design hypothesis and product goals.</p>
<h2 class="govuk-heading-s">When to set success metrics</h2>
<p class="govuk-body">You need to outline your hypothesis and set success metrics before new features are made 'live' in your service. If you wait until something had already gone live you will have missed the opportunity to measure the immediate impacts, and it will be very difficult to accurately build a 'before' and 'after' picture.</p>
<p class="govuk-body">If something has already gone live you should still engage a performance analyst, but you should try to avoid this happening.</p>
<h2 class="govuk-heading-s">How to find a performance analyst</h2>
<p class="govuk-body">The best way to define success metrics and collect data is by working closely with a performance analyst, or other data science professional. You may have dedicated performance and data professionals in your area of work or you may have to seek support from the wider Data Practice.</p>
<p class="govuk-body">You can find a performance analyst in your area on the DPA community organisational chart</p>
<h3 class="govuk-heading-s">When to engage with a performance analyst</h3>
<p class="govuk-body">Earlier is better. If you engage with performance analysts early, while you are developing your hypotheses for a design, they can help identify success metrics early in the process and plan what data collection will be necessary in advance.</p>
<p class="govuk-body">If you delay engaging a performance analyst until just before a design is about to go live then everyone, including you, will be rushed to try and work out what needs to be done before release.</p>
<p class="govuk-body">If you wait until after something has been released you will have already missed the opportunity to gain meaningful insights and it will be difficult, perhaps impossible, to retrospectively plan effective success metrics.</p>
<h3 class="govuk-heading-s">Examples of when to engage</h3>
<p class="govuk-body">Exactly how you engage with your performance analyst will depend on how your individual team approaches work, but points in which to invite a performance analyst include when you:</p>
<ul class="govuk-list govuk-list--bullet">
    <li>first receive a 'ticket' to start work on a new feature</li>
    <li>are developing hypotheses or examining user needs</li>
    <li>are sketching or wire-framing early design ideas</li>
    <li>analyse or present the outputs of user research</li>
    <li>finalise a new design or prototype for a user journey</li>
    <li>know a ticket is being prepared for the developers to implement a new design</li>
</ul>
<h3 class="govuk-heading-s">What happens after you engage</h3>
<p class="govuk-body">You'll talk about what you're delivering and work out how best to measure performance. How you do this is up to you and the performance analyst - it could include conversations, workshops, prototype walkthroughs or sharing design documentation.</p>
<p class="govuk-body">Once those success measures are agreed the performance analyst will source the data. If data is not immediately availably they'll either:</p>
<ul class="govuk-list govuk-list--bullet">
    <li>raise a request for a technical build to collect it</li>
    <li>raise a request for a manual data collection</li>
    <li>raise a risk to the team that insight cannot be gathered to measure the feature</li>
</ul>
<p class="govuk-body">When enough data has been collected the performance analyst will report the impact of the change back to the team.</p>